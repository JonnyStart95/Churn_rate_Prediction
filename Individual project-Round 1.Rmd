---
title: "Business Simulation-Round 1"
author: "MINGLIANG WEI"
date: "23/10/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("plyr")
library(dplyr)
library(tidyselect)
source("Theme4-functions.R")
```
## R Markdown
Our strategy is to find the people who has the most possiblity to leave and invite them to prevent them from leaving.Firstly, I have checked the data and find out that there are so many missing values there.
```{r r,results='hide'}
streamraw=read.csv("Retention_train.csv")
summary(streamraw)
```
To avoid missing values will cause a misleading to the regression. We will give the values of the NA obeying the caracteristic of those variables who have missing values.(Average, Maximum,Medium based on the real business meaning environment) and changing those factors varibales into factor format.
```{r}
streamraw$timeSinceLastTechProb[is.na(streamraw$timeSinceLastTechProb)]=100
streamraw$minutesVoice[is.na(streamraw$minutesVoice)]=200
streamraw$timeSinceLastIsOverData[is.na(streamraw$timeSinceLastIsOverData)]=80
streamraw$timeSinceLastIsOverVoice[is.na(streamraw$timeSinceLastIsOverVoice)]=30
streamraw$timeSinceLastComplaints[is.na(streamraw$timeSinceLastComplaints)]=100
streamraw=streamraw%>%
  mutate(isWorkPhone=as.factor(isWorkPhone), unlimitedVoice=as.factor(unlimitedVoice), 
         unlimitedText=as.factor(unlimitedText),promo=as.factor(promo))
streamraw[is.na(streamraw)]=0
```
Set the artificial variable 'Freq' which means the number of people who are using the plan in the whole family to simulate the factor of conformity behavior.
```{r}
Index=streamraw%>%select("ID","IDfamily")
Index_table=data.frame(table(Index$IDfamily))
streamraw <- merge(x=streamraw,y=Index_table,by.x = "IDfamily", by.y = "Var1", all.x = TRUE)
summary(streamraw$Freq)
```
Seperate the data into the training set and the testing set.
```{r}
stream=streamraw%>%
  filter(promo==0)%>%
  select(-c("promo","unlimitedText","ID","IDfamily"))
set.seed(88888)
trainID=sample(1:677933,550000)
train=stream[trainID,]
validate=stream[-trainID,]
```
Doing the binary regression
```{r warning=FALSE}
mod1=glm(churnIn3Month~.,family="binomial", data=train)
```
Doing the prediction based on our model we get from the training data
```{r warning=FALSE, paged.print=TRUE}
p1=predict(mod1,newdata=validate,type="response")
cbind(p1,validate)[sort.list(p1,decreasing=TRUE)[1:5],]
```
Adding predict_correction_p1 as our index showing the prediction correction rate of our model
```{r}
cbind1=cbind(p1,validate)[sort.list(p1,decreasing=TRUE)[1:10000],]
predict_correction_p1=sum(cbind1$churnIn3Month)/10000
predict_correction_p1
```
The correction rate for this model is too small for a good binory model,after checking the previous dataset.
```{r}
leaving_rate=sum(streamraw$churnIn3Month)/nrow(streamraw)
leaving_rate
```
We find out that only 2.7% percentage of people will leave in three month, and the most largest possibiliy 12.4% combing the 7.12% prediction correction rate, plus we are not sure if we invite them to come to our dinner event will help to change their mind from leaving, so we modify our strategy from inviting those who has the largest possibilities to finding the expectation money we will lost for each person, it means that we will take the potential of each customer into consideration.   
We will use the equation to caculate the potential value of each customer as follows:   
$PotentialValue=baseMonthlyRateForPlan+(baseMonthlyRateForPhone+cashDown+phonePrice+phoneBalance)^{1/2}$  
$ExpectationLossingValue=PotentialValue*Probability$  
Processing the score data.
```{r include=FALSE}
streamscore=read.csv("Retention_score.csv")
streamscore%>% nrow()
str(streamscore)
summary(streamscore)
Index_score=streamscore%>%select("ID","IDfamily")
Index_table_score=data.frame(table(Index_score$IDfamily))
streamscore <- merge(x=streamscore,y=Index_table_score,by.x = "IDfamily", by.y = "Var1", all.x = TRUE)
streamscore$timeSinceLastTechProb[is.na(streamscore$timeSinceLastTechProb)]=100
streamscore$minutesVoice[is.na(streamscore$minutesVoice)]=200
streamscore$timeSinceLastIsOverData[is.na(streamscore$timeSinceLastIsOverData)]=100
streamscore$timeSinceLastIsOverVoice[is.na(streamscore$timeSinceLastIsOverVoice)]=30
streamscore$timeSinceLastComplaints[is.na(streamscore$timeSinceLastComplaints)]=100
streamscore[is.na(streamscore)]=0
streamscore=streamscore%>%
  mutate(isWorkPhone=as.factor(isWorkPhone), unlimitedVoice=as.factor(unlimitedVoice), 
         unlimitedText=as.factor(unlimitedText))
```
Using the equation we mentioned above to predict the expectation money we will lost for each person and find the largest 8000 ones.  
We will filter those people whose Freq is 1 because based on the rule of conformity behavior, the one who has the least degree pf conformity behavior will more likely to leave.
```{r warning=FALSE, paged.print=TRUE}
p1_score=predict(mod1,newdata=streamscore,type="response")
p1_score_cbind=cbind(p1_score,streamscore)
p1_score_cbind=p1_score_cbind%>%
  mutate(expectation_value_p1=(baseMonthlyRateForPlan+(baseMonthlyRateForPhone+cashDown+phonePrice+phoneBalance)^0.5)*p1_score)
p1_score_cbind=p1_score_cbind[sort.list(p1_score_cbind$expectation_value_p1,decreasing=TRUE)[1:nrow(p1_score_cbind)],]
p1_score_cbind=p1_score_cbind%>%
  filter(Freq<=1)
head(p1_score_cbind)[1:5,]
```
